# ğŸ¤– AI Customer Support Bot

A **smart, AI-powered customer support chatbot** designed to efficiently handle user queries using **Retrieval-Augmented Generation (RAG)**. It maintains conversational context, retrieves relevant answers from a knowledge base, and intelligently escalates unresolved queries to human agents.

---

## ğŸŒŸ Features

* ğŸ§  **Intelligent Responses** â€“ Powered by Googleâ€™s **Gemini Pro** model for accurate and natural language understanding.
* ğŸ’¬ **Session Management** â€“ Maintains chat history and user context throughout the conversation.
* ğŸ“š **Knowledge Base Integration (RAG)** â€“ Retrieves answers from a predefined FAQ dataset (`faqs.json`).
* ğŸš¨ **Smart Escalation** â€“ Escalates complex queries to human agents with a summarized conversation context.
* âš™ï¸ **Robust Backend** â€“ Built with **Python** and **FastAPI** for speed and scalability.
* ğŸ³ **Dockerized** â€“ Easy to deploy using **Docker** and **Docker Compose**.

---

## ğŸ“ Project Structure

```
/
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py             # FastAPI application and endpoints
â”‚   â”‚   â”œâ”€â”€ models.py           # SQLAlchemy database models
â”‚   â”‚   â”œâ”€â”€ schemas.py          # Pydantic validation models
â”‚   â”‚   â”œâ”€â”€ database.py         # DB config and session management
â”‚   â”‚   â”œâ”€â”€ db_operations.py    # Database CRUD operations
â”‚   â”‚   â”œâ”€â”€ ai_service.py       # Core AI logic and LLM interactions
â”‚   â”‚   â”œâ”€â”€ faq_service.py      # Knowledge base search logic
â”‚   â”‚   â””â”€â”€ prompts.py          # Central LLM prompt templates
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ faqs.json           # FAQ knowledge base
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ test_api_phase3.py  # Automated tests
â”‚   â”œâ”€â”€ .env                    # Environment variables
â”‚   â””â”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ client/                     # (Optional frontend)
â”œâ”€â”€ docker-compose.yml          # Docker Compose configuration
â””â”€â”€ README.md                   # This file
```

---

## ğŸš€ Getting Started

### ğŸ§© Prerequisites

* [Docker](https://www.docker.com/get-started)
* [Docker Compose](https://docs.docker.com/compose/)

### ğŸ› ï¸ Setup Instructions

1. **Clone the repository**

   ```bash
   git clone <your-repo-url>
   cd <repository-folder>/server
   ```

2. **Add environment variables**
   Create a `.env` file inside `server/` and add your Gemini API key:

   ```bash
   GEMINI_API_KEY="YOUR_GOOGLE_AI_API_KEY"
   ```

3. **Run with Docker Compose**
   From the root directory:

   ```bash
   docker-compose up --build
   ```

   The API will be live at **[http://localhost:8000](http://localhost:8000)**.

---

## ğŸ§  Prompt Engineering Strategy

This project uses a **three-stage triage system** to ensure fast, relevant, and accurate responses.

### ğŸ”¹ Stage 1 â€“ Small Talk Filtering

Quickly detects and responds to casual messages (e.g., â€œHiâ€, â€œThanksâ€) without calling the LLM.

### ğŸ”¹ Stage 2 â€“ RAG with LLM Validation

1. Searches `faqs.json` for related context.
2. Validates the relevance using a Yes/No LLM prompt.
3. If valid, uses the context to generate the final answer.

### ğŸ”¹ Stage 3 â€“ Intelligent Escalation

If no suitable context is found, the bot creates a short summary and escalates to a human agent.

---

## ğŸ§© Core Prompt Templates

### âœ… Context Validation Prompt

```python
CONTEXT_VALIDATION_PROMPT_TEMPLATE = """
You are an AI assistant that determines if a provided context is relevant for answering a user's question.

User Question: "{question}"
Provided Context: "{context}"

Does the provided context contain enough information to directly answer the user's question?
Respond with only "YES" or "NO".
"""
```

### ğŸ“š Retrieval-Augmented Generation (RAG) Prompt

```python
RAG_PROMPT_TEMPLATE = """
{system_prompt}

Based on the following conversation history and provided context, answer the user's question.

**Conversation History:**
{chat_history}

**Knowledge Base Context:**
{context}

**User's Question:**
{question}

**Your Answer:**
"""
```

### ğŸ§¾ Summarization Prompt

```python
SUMMARIZATION_PROMPT_TEMPLATE = """
Based on the following conversation history, provide a concise, one-sentence summary of the user's issue for a human agent.

**Conversation History:**
{chat_history}

**Summary:**
"""
```

---

## ğŸ§ª Testing

This project includes automated tests using **pytest**.

Run tests using:

```bash
pytest -v -s server/tests/test_api_phase3.py
```

Ensure that all Docker containers are running before executing tests.

---

## ğŸ§° Tech Stack

* **Backend:** FastAPI (Python)
* **Database:** PostgreSQL (via SQLAlchemy)
* **AI Model:** Google Gemini Pro
* **Containerization:** Docker & Docker Compose
* **Testing:** pytest

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ’¡ Contributing

Contributions are welcome! Feel free to open an issue or submit a pull request.

---

### â­ If you like this project, give it a star on GitHub!
